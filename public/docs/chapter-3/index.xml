<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chapter 3: Building and Running the System on Pokemon RL</title>
    <link>http://localhost:1313/pokerl/docs/chapter-3/</link>
    <description>Recent content in Chapter 3: Building and Running the System on Pokemon RL</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/pokerl/docs/chapter-3/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Running</title>
      <link>http://localhost:1313/pokerl/docs/chapter-3/running/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/pokerl/docs/chapter-3/running/</guid>
      <description>&lt;h1 id=&#34;actually-building-the-rl-system&#34;&gt;&#xA;  Actually Building the RL System&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#actually-building-the-rl-system&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;So I have a policy, observations, an environment and a reward. Now it&amp;rsquo;s time to discuss the meat of the system and how I wrote it. Along the way, I&amp;rsquo;ll provide relevant snippets of code and tricks I did to make my system run as fast as possible.&lt;/p&gt;&#xA;&lt;p&gt;Before I begin though, I am obligated to give thanks to Joseph Suarez, the creator of PufferAI and PufferLib. Joseph graciously donated 4 machines to this effort. Each machine contained a NVIDIA 4090 GPU, Intel Core i9-14900K, 125 GB of RAM and 2TB of disk space. Before his contribution, I was paying $200/month using &lt;a href=&#34;https://vast.ai/&#34;&gt;vast.ai&lt;/a&gt; for training with worse hardware.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reading RAM</title>
      <link>http://localhost:1313/pokerl/docs/chapter-3/reading-ram/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/pokerl/docs/chapter-3/reading-ram/</guid>
      <description>&lt;h1 id=&#34;reading-ram&#34;&gt;&#xA;  Reading RAM&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#reading-ram&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;</description>
    </item>
    <item>
      <title>Metrics and Visualization</title>
      <link>http://localhost:1313/pokerl/docs/chapter-3/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/pokerl/docs/chapter-3/metrics/</guid>
      <description>&lt;h1 id=&#34;metrics&#34;&gt;&#xA;  Metrics&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#metrics&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;</description>
    </item>
    <item>
      <title>The Swarm</title>
      <link>http://localhost:1313/pokerl/docs/chapter-3/swarm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/pokerl/docs/chapter-3/swarm/</guid>
      <description>&lt;h1 id=&#34;the-swarm&#34;&gt;&#xA;  The Swarm&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#the-swarm&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;I know I haven’t gotten to the final run yet. I have not mentioned an important modification I made to the normal PPO loop. Pokemon is a nearly open world game. Unfortunately, the experiential data can get “non-coherent” and the agents can become detached from each other.&lt;/p&gt;&#xA;&lt;p&gt;Without sufficiently coherent data, the policy will not improve. This plagued me for &lt;em&gt;months&lt;/em&gt;. Eventually, I took inspiration from the &lt;a href=&#34;https://arxiv.org/abs/1901.10995&#34;&gt;Go-Explore&lt;/a&gt; paper. I changed the way training is done. I made assurances that the data would remain coherent.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
