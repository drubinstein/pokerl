[{"id":0,"href":"/pokerl/docs/chapter-1/the-loop/","title":"RL Quickstart","section":"Chapter 1: RL and the Pokémon Environment","content":" RL Quickstart # Before I focus on Pokémon, I\u0026rsquo;d like to start by giving a brief primer on Reinforcement Learning. Reinforcement Learning (RL) is a focus of study on how an agent should take actions in an environment in order to maximize a reward. This primer will not be exhaustive. I am writing it to set the framework for which I built my Pokémon training system.\nIn a typical RL framework, there will be 2 components.\nThe Environment - Represents the world context. The Agent - Responsible for generating actions to perform in the environment The agent contains a policy for generating an action given a state of the environment using a . The environment evaluates the action and returns an observation and a reward. These actions, observations and rewards can be used to update the agent\u0026rsquo;s policy. Deep Reinforcement Learning (DRL) uses a neural network for the policy. I\u0026rsquo;ll be using DRL for Pokémon.\n--- config: theme: mc look: handDrawn layout: elk --- flowchart LR Agent --\u003e |action| Environment(Environment) Environment --\u003e |Observation, Reward| Agent subgraph Agent[Agent] Policy(Policy) end When building an RL system, I like to breakdown the problem into modeling the problem as\nthe goal the environment the observation the actions the reward the policy / the agent Tic Tac Toe # Let\u0026rsquo;s apply RL to Tic-Tac-Toe. RL does not need machine learning. A valid RL agent can be as simple as a look up table! For those who have never played Tic-Tac-Toe, Tic-Tac-Toe is a two player game where each player takes turns marking a single square in a 3x3 grid. The first player to have marked 3 squares making a horizontal, vertical or diagonal line wins. Let\u0026rsquo;s breakdown Tic-Tac-Toe in the framework above\nExample Tic-Tac-Toe configurations: X Wins Tie Early Game X | O | X X | O | X X | | O ----------- ----------- ----------- O | X | O X | X | O | O | ----------- ----------- ----------- | | X O | X | O | | X Goal # The goal of Tic-Tac-Toe is to win! After a player has achieved 3-in-a-row, the game will reset. We call this beginning of game to end of game period an episode.\nEnvironment # The environment is the Tic-Tac-Toe game itself. The environment will be responsible for performing actions on the grid, i.e., marking squares, ensuring invalid moves are not played and determining when the game has ended.\nObservation # The observation will be the current 3x3 grid represented as a 3x3 array.\nActions # The action will be a coordinate to play on the grid. For example [0, 0] will mean mark on the upper left hand corner. [2, 2] will mean mark the lower right hand corner.\nRewards # We can start with a reward of\n+1 if the player wins -1 if the player loses We can even be simpler and say +1 if the player wins, -1 if the player loses. The act of modifying the reward for a better system is known as reward shaping. If you make your reward simpler, you risk the agent never learning how to perform complex actions. If you make your reard complex, you risk the agent will never progress beyond immediate greedy actions.\nTraining the Policy # Now that we have created our Tic-Tac-Toe RL system, we can attempt to train a policy to be the best Tic-Tac-Toe player ever. At the start of training, the policy will play random moves, but as it obtains more experience, it will learn to exploit the reward function.\nFor Pokémon, I am going to breakdown the game similarly. Let\u0026rsquo;s dig in.\n"},{"id":1,"href":"/pokerl/docs/chapter-1/env-setup/","title":"Setting up the Environment","section":"Chapter 1: RL and the Pokémon Environment","content":" Setting up the Environment # When I start designing an RL system, I start with the environment. The Python library Gymnasium provides a fairly straightforward API which can be simplified into two functions: Reset and Step\nResets, Episodes and the Goal # Reset handles the initialization for a gameplay session also known as an “episode.”\nIn most video game DRL projects I\u0026rsquo;ve observed a reset will happen when an agent\nAchieves a major milestone, such as defeating a boss. Encounters a failure state, such as losing all their lives. Reaches a predefined time or action limit, such as 100 tetriminos in Tetris, Pokémon is in the realm of “long episodic RL.” Long episodes mean that the agent may not get large rewards for a very long time. If the agent only receives large rewards at the end of an episode, it may have trouble creating long term policies.\nImagine if in the Tic-Tac-Toe example, the grid was 100x100. That means the agent wouldn\u0026rsquo;t receive a reward for up to a max 5000 steps. Imagine having to plan out a 5000 step strategy. It\u0026rsquo;s not easy! Later, I\u0026rsquo;ll go over how I handled rewards for Pokémon\u0026rsquo;s long episodes.\nBased on prior work, I began with an episode being a fixed number of steps. Over time, I tried multiple additional strategies including dynamically increasing the number of steps per episode as the agent performed important milestones. However, I believe an episode should be the based on a goal or when the agent achieves an unrecoverable state (soft-lock), e.g., such as running out of money.\nWhat\u0026rsquo;s the goal of Pokémon I aimed to complete? To be the champion! Therefore, I came up with a compromise. I created “mini-episodes.” An episode would be the duration of an entire game. However, the agent\u0026rsquo;s state would periodically reset mid-episode, but the emulator state would not.\nSteps # Step handles actions meant for the environment and returns observations based on a taken action, any logging info and whether or not to reset the environment.\nOur step function for Pokémon can naively be written as:\nReceive a button press action Send the button press to the gameboy emulator Wait some frames for the action to have an effect on the environment Sample the environment Return data based on the sample And that’s really it. Once I had the game loop running in a Gymnasium Environment, I could begin to implement game-specific functionality.\n"},{"id":2,"href":"/pokerl/docs/chapter-1/breakdown/intro/","title":"Intro","section":"Breaking Down Pokémon","content":" Breaking Down Pokémon # Before going into observations, rewards and the policy, I believe studying the environment is higher priority. As I already said multiple times, Pokémon is a complex game with multiple tasks and puzzles that can be accomplished nonlinearly.\nOnce we have an understanding of the game and all its gotchas, we can engineer observations and rewards to accomplish all tasks.\nTo begin, I enumerated the game’s storyline objectives. These objectives may be a little more detailed than what you would read in an average walkthrough, but it was important to cover what’s required or risk the agent getting stuck. At a high level, beat Pokémon, you must:\nBeat the 8 Gym Leaders. Gym leaders are a form of video game “boss.” Acquire items to teach the moves CUT, STRENGTH, and, SURF. Field moves are abilities that can be used outside of battle to unlock a new area or make it easier to traverse an existing area. Acquire Pokémon that can learn the field moves CUT, STRENGTH, and SURF. Acquire any items required for field interactions. Like field moves, there are items that are required to unlock new areas. Teach available Pokémon the field moves CUT, STRENGTH, and SURF Use field moves or items for field interactions to remove any game blocking obstacles. Complete the Team Rocket storyline. Beat the 6 required rival battles Beat the Elite 4 and Champion Or, if we want to be more detailed\u0026hellip;\n--- config: theme: mc look: handDrawn layout: elk --- flowchart TD AA(Acquire starter Pokémon from Prof. Oak) AA --\u003e AAA(Acquire Parcel from Viridian Mart) --\u003e AAAA(Deliver Parcel to Prof. Oak) AAAA --\u003e A(Defeat Brock) --\u003e B(Traverse Mt. Moon) B --\u003e C(Defeat Misty) B --\u003e D(Nugget Bridge) --\u003e E(Get the SS Anne ticket from Bill) --\u003e F(Defeat Cerulean Rocket Grunt) --\u003e G(Defeat Rival on the SS Anne) --\u003e H(Obtain HM01 - Cut from the Captain on the SS Anne) C --\u003e I C --\u003e J C --\u003e K C --\u003e M H --\u003e HH(Defeat Rocket grunt protecting Rocket Hideout) --\u003e HHH(Flip switch in Celadon Game Corner to unlock the Rocket Hideout) HHH --\u003e KK(Defeat Rocket grunt with Lift Key) --\u003e KKK(Obtain the Lift Key) --\u003e K(Defeat Giovanni in Rocket Hideout) K --\u003e LL(Collect the Silph Scope from Giovanni) --\u003e LLL(Use the Silph Scope on the ghost in Lavender Tower) LLL --\u003e L(Save Mr. Fuji at the top floor of Lavender Tower) H --\u003e I(Defeat Lt. Surge) H --\u003e J(Defeat Erika) H --\u003e MM(Obtain a drink from the vendeing machines at the top of Celadon Mart) MM --\u003e M(Deliver Drink to Saffron Guard) --\u003e O L --\u003e N(Obtain Pokéflute) L --\u003e OO(Defeat Rival 5 in Silph Co) --\u003e O(Defeat Giovanni in Silph Co) --\u003e P(Defeat Sabrina) N --\u003e Q(Use Pokéflute on at least one Snorlax) Q --\u003e R(Obtain HM03 - Surf from the Safari Zone) --\u003e T(Acquire the Secret Key from Pokémon Mansion) --\u003e U(Defeat Blaine) Q --\u003e V(Defeat Koga) --\u003e T Q --\u003e SS(Obtain the Gold Teeth from the Safari Zone) --\u003e S(Deliver the Gold Teeth to the old man in Fuchsia City to acquire HM04 - Strength) I --\u003e W(Defeat Giovanni in Viridian Gym) --\u003e X(Defeat Rival 6) --\u003e Y(Traverse Victory Road) J --\u003e W P --\u003e W U --\u003e W S --\u003e Y Y --\u003e Z(Defeat the Elite 4) --\u003e ZZ(Defeat the Champion) I broke down these objectives to understand what is and is not important for beating Pokémon Red.\n"},{"id":3,"href":"/pokerl/docs/chapter-1/breakdown/battling/","title":"Battling","section":"Breaking Down Pokémon","content":" Battling # Half of the previously mentioned objectives involve battling and winning against other trainers. However, I concluded that teaching battling is not required to make an agent capable of completing Pokémon with RL.\nThe Pokémon battle system is pretty straight forward. Pokémon is an advanced game of rock-paper-scissors. Pokémon have moves that can be used to either inflict status effects against other Pokémon, damage other Pokémon or boost their own abilities or self-heal. The strength of each move is affected by the strengths and weaknesses of each Pokémon relative to the move and overall stats. Moves are given priority based on a Pokémon\u0026rsquo;s SPEED stat or based on a move\u0026rsquo;s priority. Damage is based on the Pokémon\u0026rsquo;s ATTACK, SPECIAL and DEFENSE stats depending on the move. Additionally, moves can miss based on a move and Pokémon\u0026rsquo;s ACCURACY stats.\nDuring battles, the agent can additionally switch their current Pokémon in-battle or use items to heal their Pokémon.\nThere are two types of battles in Pokémon. The first type are wild battles where the agent will battle against a single Pokémon. Wild battles generally begin with a random encounter in grassy areas or dungeons. The second type are trainer battles. Trainer battles place the agent against a team of one or more Pokémon. Trainers have access to the same item and party switch actions the agent has.\nBattles end when all Pokémon on one side have fainted. Fainting is when a Pokémon\u0026rsquo;s health drops to zero. Additionally, during a wild battle, the agent has the option to run and leave the wild battle without penalty or catch the opposing Pokémon and have the opposing Pokémon join their party.\nIf the agent\u0026rsquo;s Pokémon knocks out the opposing Pokémon, they gain experience (EXP). With enough EXP, a Pokémon will level up. Levelling up provides a mechanism to increase the Pokémon\u0026rsquo;s stats. Levelling up to increase stats gives a mechanism to ignore worrying about battling. If the agent “grinds,” that is, let Pokémon level up or retry trainers infinite times, the agent will eventually win outside of a couple of very unlikely situations.\nConsequentially, I ignored creating a policy for battling and focused on providing suitable conditions for levelling up when needed.\n"},{"id":4,"href":"/pokerl/docs/chapter-1/breakdown/8badges/","title":"The 8 Badges","section":"Breaking Down Pokémon","content":" The 8 Badges # Next, I focused on what it takes to beat the 8 gym leaders. The gym leaders upon defeat, give the main character a “badge.” Badges either unlock the ability to use a taught field move or apply permanent stat modifiers to the agent’s party. Below is a table of the gym leaders of Pokémon, their types and what they unlock.\nGym Leader Type Reward Brock\nRock - 12.5% ATTACK boost- Can use FLASH outside of battle Misty\nWater - Can use CUT outside of battle Lt. Surge\nElectric - 12.5% DEFENSE boost- Can use FLY outside of battle Erika\nGrass - Can use STRENGTH outside of battle Koga\nPoison - 12.5% SPEED boost- Can use SURF outside of battle Sabrina\nPsychic Blaine\nFire -12.5% SPECIAL boost Giovanni\nGround I emphasized 5 gym leaders. Why? Because these gym leaders can be accomplished in nearly any order. The vast majority of storyline events are gated by Misty, Erika, and Koga who give access to the field moves CUT, STRENGTH and SURF outside of battle. How to obtain these field moves will be discussed in a later section.\nSome gym leaders require solving puzzles or removing environmental guards.\nLt. Surge\u0026rsquo;s Gym Gimmick # To enter Lt. Surge’s gym you must remove a tree blocking the entrance to his gym with Cut.\nLt. Surge’s gym contains a button puzzle. To acquire the ability to battle Lt. Surge, you are given an array of trashcans. Two of the trashcans have a button that can be pressed. You must press the two buttons in a specific order. If you mess up, the buttons are randomly assigned to different trashcans.\nErika\u0026rsquo;s Gym Gimmick # To enter Erika\u0026rsquo;s gym you must remove a tree blocking the entrance to her gym with cut. In her gym, she will be guarded by multiple trainers you must defeat and more trees to cut. Notably, the sprite for the trees within the gym are different from the tree sprite outside the gym.\nKoga\u0026rsquo;s Gym Gimmick # Koga’s gym contains an easy maze made up of invisible barriers you have to traverse.\nSabrina\u0026rsquo;s Gym Gimmick # Sabrina’s gym can only be entered after completing the Team Rocket storyline. Her gym contains a teleporter maze. To get to Sabrina, you must find the teleporter that will transport you to her.\nBlaine\u0026rsquo;s Gym Gimmick # To enter Blaine’s gym the agent must first Surf to Cinnabar Island where the gym is located. Subsequently, the agent must acquire the “SECRET KEY” from the Pokémon Mansion next door to his gym Surf. To battle Blaine, the agent must complete a series of optional True/False quizzes. If the agent incorrectly answers, the agent battles a trainer.\nGiovanni\u0026rsquo;s Gym Gimmick # Giovanni’s gym is blocked until you beat all 7 prior gym leaders. Giovanni’s gym contains a maze puzzle with “spin tiles.”\nThe agent must be capable of solving all these tasks.\n"},{"id":5,"href":"/pokerl/docs/chapter-1/breakdown/field_interactions/","title":"Field Interactions and Exploration","section":"Breaking Down Pokémon","content":" Field Interactions and Exploration # As mentioned previously, field moves taught to Pokémon via “HMs” allow you to interact with the world. These HMs can only be taught to specific Pokémon.\nThe 5 field moves of Pokémon are\nHM01 - CUT HM02 - FLY HM03 - SURF HM04 - STRENGTH HM05 - FLASH Only CUT, SURF, and STRENGTH are required for game progression. Although useful, FLY and FLASH aren’t required. They only allow for faster game progression.\nUsing an HM # Teaching and using an HM requires a minimum of 17 actions along with a Pokémon in the agent\u0026rsquo;s party that is capable of learning the HM. 17+ actions are hard enough to give the agent context for. Incentivizing the agent to have HM capable party members when it is playing Pokémon for the first time without a guide is an even harder task!\nObtaining HM01 - CUT # To acquire HM01, the agent must:\nAcquire the SS Anne ticket from Bill north of Cerulean City. Board the SS Anne and battle their rival Help the captain of the ship. The captain will reward the agent with the item HM01.\nObtaining HM03 - SURF # To obtain SURF, the agent must reach the end of the Safari Zone in less than 500 steps. At the end, the Safari Zone Warden will reward the player with HM03\nObtaining HM04 - Strength # To obtain STRENGTH, the agent must:\nFind the GOLD TEETH in the Safari Zone. Bring the GOLD TEETH to the old man NPC in Fuchsia City The old man will then reward the agent with the item HM04\nObtaining and Using the Pokéflute # Along with field moves, the agent will need to use the item Pokéflute for one field interaction. Pokéflute can remove two roadblocks (at least one is required) so the agent can gain access to Fuchsia City.\nHowever, using Pokéflute is very similar to using an HM. A level of convenience that I will use when ensuring the agent can perform the action.\n"},{"id":6,"href":"/pokerl/docs/chapter-1/breakdown/team_rocket/team_rocket/","title":"Team Rocket","section":"Breaking Down Pokémon","content":" Team Rocket Storyline # Beyond the gym battles and field moves, there are mandatory quests involving encounters with the antagonist group, Team Rocket. These events require defeating trainers, obtaining special items, and solving in-game puzzles. Thankfully, these quests must be accomplished in a linear order. For completeness, hare are all the tasks required to to complete the Rocket storyline:\nDefeat the rockets blocking the exit to Mt. Moon (post Brock). Defeat the rocket at the end of Nugget Bridge north of Cerulean City. Defeat the rocket at the exit of Cerulean City. Defeat the rocket guard in the Celadon City Casino. Press the poster switch the Celadon City Casino Rocket guard is guarding and enter the Rocket hideout. Solve the Rocket hideout maze puzzle on BF3 to get to BF4. Beat the Rocket grunt in BF4. Pick up the dropped elevator key from the BF4 Rocket Grunt. Solve the maze puzzle on BF2. Use the elevator key to enter the elevator on BF2. Go to BF4 using the elevator key. Beat Giovanni in Rocket Hideout. Pick up the Silph Scope Giovanni drops. Go to the Pokémon Tower in Lavender Town. Use the Silph Scope to defeat the ghost waiting at the staircase to the second-to-top floor of Lavender Tower. Save Mr. Fuji at the top of Lavender Tower . Obtain a drink from the vending machine of Celadon City (can be done any time before this). Give a drink to one of the 4 Saffron City gate guards (can be done any time after obtaining the drink). Find the card key in Silph Co in Saffron City. Defeat Giovanni in Silph Co. It’s a lot of tasks. Unlike the gym leaders, the difficulty of the Team Rocket puzzles are highly variable, but brute-forceable. From these quests, I concluded the agent needed a desire to either explore the whole world or spend extra time building a policy capable of reading and interpreting text. I decided to focus on the former.\n"},{"id":7,"href":"/pokerl/docs/chapter-1/breakdown/route/","title":"The \"Route\"","section":"Breaking Down Pokémon","content":" Defining a \u0026ldquo;Route\u0026rdquo; # For the agent to complete all objectives, I wanted to simplify the number of game as much as possible to maximizing the likelihood of success. To limit non-determinism, I started the agent after the \u0026ldquo;Parcel Delivery\u0026rdquo; event. Starting the agent with a specific Pokémon would guarantee later stages of the game would be possible. Given the previous breakdown, here’s the route I wanted the agent to learn:\nStart with Bulbasaur or Charmander to guarantee a Pokémon who can use CUT. Defeat Brock. In any order: Defeat Misty. Acquire HM01 and teach CUT to Bulbasaur. In almost any order: Defeat gyms 3-7. Acquire HM03 and HM04. Complete the Team Rocket Storyline. Acquire the gift Lapras in Silph Co and teach the Lapras Surf and Strength. Defeat Gym 8. Defeat the 6th rival battle. Defeat the Elite 4 + Champion. "},{"id":8,"href":"/pokerl/docs/chapter-1/breakdown/risk_management/","title":"Risk Management","section":"Breaking Down Pokémon","content":" Risk Management # Even though I simplified the game, I still needed risk mitigation. Pokémon contains numerous game ending risks including:\nPermanently losing vital Pokémon Catching Pokémon and not having enough space for the Lapras or any other Pokémon that can learn Surf Item management. If the agent obtains too many items, then there may not be enough room for key items Money management. It is possible to soft-lock if you cannot obtain any more money at the time the Safari Zone objectives need to be completed. Only having Pokémon with non-damaging moves. I’d like to emphasize again that none of these issues require teaching the agent how to best Pokémon battles.\nScripting vs. Emergence # To handle these risks and difficulties, I split agent behavior into two classes:\nScripted: Meaning actions taken by the agent not controlled by the policy. Emergent: Allowing the agent to discover its own strategies. Ideally, no scripted behavior would be needed. However, I needed scripts. I aimed to only script behaviors that require human intuition or tasks that are not really a core part of Pokémon. These included.\nItem management - the agent will toss all non-key items if the agent fills their inventory. Money management - the agent will have infinite money. Solving puzzles that require Strength. Blocking the Indigo Plateau exit at the end of the game. To make development easier, I wrote scripts to remove the following complexity and speed up development. These scripts can disabled for a successful run, but were invaluable during development. They included\nTeaching HMs. Using HMs outside of battle. Using Pokéflute. Maxxing the Pokémon’s stats. Automatic insertion of drinks for the Saffron guards if the agent entered the Celadon Mart. Automating elevator usage. If an agent entered an elevator, the elevator would go to the next floor modulo number of floors. Disabling wild battles to simplify dungeons. "},{"id":9,"href":"/pokerl/docs/chapter-2/observations/","title":"Observations","section":"Chapter 2: Observations, Rewards, and Policy","content":" Observations # As mentioned previously, observations are a representation of the state of the environment. In the Tic-Tac-Toe example, the observation was a 3x3 grid. For Minecraft, it could be a screen image and the agent\u0026rsquo;s current inventory. Pokémon contains tons of visible and invisible pieces of information.\nIt is possible to give the agent the entirety of in-game RAM and let the agent solve the game. I desired to make the agent play the game as a new player would. When in doubt, I tried to stick to one rule when designing my observations:\nThe observation can not contain any in-game knowledge a human player would not have access to.\nWhat does this mean in practice? No knowledge of a Pokémon’s hidden stats. No knowledge of a Pokémon’s future moves. No knowledge of an enemy’s moveset. No knowledge of the whole game map until a new area was visited. etc.\nMy First Observations # The current game screen downsampled by 2 (72x80 pixels) in grayscale. # Pokémon Red was released as a grayscale game. The color information does not provide any extra information until later generations. Downsampling the screen still provided enough information to determine where the character is. The screen gives the agent the most direct knowledge of what it is doing. An alternative would be to collect all entity information for the current screen from RAM. However, I felt that the game screen provided the best parallel to how a human interacts with a Gameboy Another alternative would be to provide the sprites as an observation. Again, I like how using pixels mimics how a human would play Pokémon Red. The \u0026ldquo;Visited Mask\u0026rdquo; # The Visited Mask is a view of the current game screen displaying where the player has not visited in its mini-episode. The visited mask provides a way of giving the agent its exploration history to the policy. Without the visited mask, the agent was more likely to revisit the same areas repeatedly and never progress A binary vector of all events the agent has or has not completed. # Events are in-game objectives that have been accomplished. In the GameBoy\u0026rsquo;s WRAM, the events array tells the game what to enable/disable when the agent enters a new map. I tried as hard as possible to remove this. I want to eventually as believe the events observation leaks information. I had to create four events not automatically included in the events array: Rival 3 defeated. Lapras acquired. Drink given to Saffron guard. Celadon Game Corner Rocket defeated. Handling New Challenges # Along the way, I added more observations to handle specific in-game complexity.\nThe direction the agent is facing # This provided an extra hint to teach the agent about what player sprite maps to what orientation. The map ID an agent will return to if they lose a battle (Blackout Map ID) # This is not necessary, but was a way to help the agent understand to use Pokémon Centers, a form of checkpointing. The items in the agent’s inventory along with quantities # If the agent ever learned about what items it could use, this is how. Some items are required. This observation helps the agent know when it obtained a required, non-event item The agent’s party including any information available in a party member’s stats page, e.g. health left # This observation may not have been necessary, I placed these observations in-case I ever wanted to write battle AI. The number of steps left in the safari zone minigame # Although I have not explained the Safari Zone in-depth, I wanted to prevent the agent from running in circles in the Safari Zone. Whether or not the agent has received the gift lapras. # Although redundant given the party observation, Lapras is extremely important for game progression per \u0026ldquo;The Route.\u0026rdquo; "},{"id":10,"href":"/pokerl/docs/chapter-2/rewards/","title":"Rewards","section":"Chapter 2: Observations, Rewards, and Policy","content":" Rewards # Rewards was where I was willing to be a bit more flexible and leak some information that an agent wouldn’t normally have access to. Only rewarding the storyline is very sparse. It can be thousands of steps before a high value event is attempted. The agent needs some way knowing it is making progress in between these high value sparse rewards.\nSparse vs. Dense Rewards # Sparse rewards were the rewards I chose that were very high value but rarely occur, e.g. gym battles. If we were to only reward the sparse rewards, the agent would most likely never leave the starting point in Pallet Town.\nDense rewards are like breadcrumbs. They are for teaching an agent how to explore or interact with the world. By mixing dense rewards in, the agent can progress and eventually discover the highly valued sparse rewards.\nIs Exploration All You Need? # Exploration became the foundation for all dense rewards.\nI originally believed that an agent had successfully learned Pokémon if the agent could visit every coordinate in the game’s world. I loosened that definition. I now consider any interaction a form of exploration.\nHowever, onnly using exploration rewards are not enough. The mix of sparse and dense rewards is what makes the policy successful! If I were to only reward for unique coordinates, the agent may never interact with Brock, the first gym leader. Instead, the agent would wander the world forever collecting new locations until it ran out of locations.\nIn the end, I rewarded for:\nThe number of unique coordinates visited during a mini-episode. The number of signs interacted the agent interacts with during a mini-episode. The number of unique moves taught. The number of valid and invalid uses of HMs/Pokéflute in the overworld. The number of unique map ID transitions (warps) visited during a mini-episode. The number of unique seen and caught Pokémon. The current level of the agent’s party with a cutoff to prevent grinding (not exploration related). I tried to also reward for all locations an agent has pressed A on and the number of unique menus the agent visited during an epiosde. However, the A press and unique menus reward provided no gain and slowed down training tremendously.\nMaking Game Progress? # Game progress was ultimately defined by high value sparse rewards. These included:\nGym badges obtained. All required events completed. All required items obtained. Without these rewards, the agent would ultimately wander until the mini-episode reset and make no progress.\nMap ID Rewards # I additionally gave the option for a \u0026ldquo;boosted\u0026rdquo; reward if the agent explored a currently important map ID. I understand this is a controversial choice. On one hand, I am potentially putting the game on rails. On the other hand, I experienced that the agent would have no clue where to go next unless I added boosted rewards for specific map IDs since the agent could not understand text. Often in Pokémon, an NPC will tell the player where to go next. The agent not being able to understand text obviously could not know where to go so I compromised.\nSpecific map IDs start the game with a boosted exploration reward until the map ID’s objective is complete. For example, a gym has boosted reward until it is beaten. Only two map IDs break this rule.\nRoute 23 does not get boosted until all gyms are beaten Lavender Tower does not get boosted until the Celadon City rocket event is completed. Otherwise, agents would not leave Lavender due to the level up reward obtained from the wild battles inside Lavender Tower. Safari Zone Rewards # The Safari Zone minigame provided a big challenge. The Safari Zone is a minigame where two required items must be obtained within 500 steps. The Safari Zone can be attempted as long as you can pay for it. Safari Zone’s route is simple to reward for until a fork in the road 75% the way in. I needed to incentivize the agent to get to that fork as fast as possible and take the correct fork.\nI added a specific Safari Zone reward for this case. The agent obtains a reward proportional to the number of steps left for any map id it reaches within the Safari Zone. Ideally, the agent’s visited mask will prompt the agent to explore both forks within the same episode. In reality, the agent does eventually complete the Safari Zone but it can take thousands of steps during training.\n"},{"id":11,"href":"/pokerl/docs/chapter-2/policy/","title":"Policy","section":"Chapter 2: Observations, Rewards, and Policy","content":" RL Algorithm and Policy # For training, I adopted the on-policy algorithm Proximal Policy Optimization (PPO). PPO supports vectorized environments well and there is a large amount of online code examples surrounding it.\nI tried to keep the policy itself simple. I wanted a policy that\nCould have some way of processing sequential (time) data. Was small for faster training. To handle time dependence I considered a few options:\nModify my observation to stack frames. Each frame would linearly increase the model’s input. Then use some form of model that handles batches of sequential data, e.g., a 3D CNN or attention layer. These models would have heavily slowed down training and require more VRAM than I had at the time Use a recurrent neural network such as an LSTM. Use a state space model (SSM). I went with the easiest to integrate solution, an LSTM. An LSTM contains an internal state that gets input to the model alongside the most recent data point. Although LSTMs can only remember up to ≈1000 steps, that was enough history for an effective policy.\nThe policy ended up being ≈10 million parameters or 40MB. For context, that\u0026rsquo;s 5 orders of magnitude smaller than DeepSeek. Enough to fit on the average consumer GPU.\nFeature Engineering the Policy # I mentioned the observation I input to the policy, but did not mention its members’ data types. As I’ll explain later, I wanted to keep the size (in bytes) of the observation as small as possible. I also needed to transform the observation into a form the model dictating the policy could use for optimization.\nPokémon Red Policy # Pokémon Red Policy --- config: theme: mc look: handDrawn --- flowchart LR Party_Network2[\"Party Network\"] FinalConcat(\"Concat\") Screen_Network2[\"Screen Network\"] MapIDE(\"Map ID Embeddings\") MapID(\"Map ID\") MapIDE2(\"Map ID Embeddings\") BlackoutMapId(\"Blackout Map ID\") Mul(\"x\") ItemIDE(\"Item ID Embeddings\") BagItemIds(\"Bag Item IDs\") div100(\"/ 100\") BagItemQ(\"Bag Item Quantities\") Events(\"Events Completed Array\") OneHot(\"One-hot\") Direction(\"Direction\") OneHot2(\"One-hot\") BattleT(\"Battle Type\") MissingEvents2(\"Missing Events\") div502(\"/ 502.0\") SafariZoneSteps(\"Safari Zone Steps Remaining\") LSTM(\"LSTM\") Linear3(\"Linear\") Party_Network2 --\u003e FinalConcat Screen_Network2 --\u003e FinalConcat MapID --\u003e MapIDE MapIDE --\u003e FinalConcat BlackoutMapId --\u003e MapIDE2 MapIDE2 --\u003e FinalConcat BagItemIds --\u003e ItemIDE ItemIDE --\u003e Mul BagItemQ --\u003e div100 div100 --\u003e Mul Mul --\u003e FinalConcat Events --\u003e FinalConcat Direction --\u003e OneHot OneHot --\u003e FinalConcat BattleT --\u003e OneHot2 OneHot2 --\u003e FinalConcat MissingEvents2 --\u003e FinalConcat SafariZoneSteps --\u003e div502 div502 --\u003e FinalConcat FinalConcat --\u003e Linear3 Linear3 --\u003e LSTM Party Network --- config: theme: mc look: handDrawn --- flowchart LR Concat(\"Concat\") SIDE(\"Species Embeddings\") SID(\"Species ID\") Hp(\"HP\") Status(\"Status\") t1e(\"Type Embeddings\") t1(\"Type 1\") t2e(\"Type Embeddings\") t2(\"Type 2\") level(\"Level\") MaxHp(\"Max HP\") Attack(\"Attack\") Defense(\"Defense\") Special(\"Special\") MovesE(\"Moves Embeddings\") Moves(\"Moves\") Flatten1(\"Flatten\") Linear2(\"Linear\") Linear1(\"Linear\") SID --\u003e SIDE SIDE --\u003e Concat Hp --\u003e Concat Status --\u003e Concat t1 --\u003e t1e t1e --\u003e Concat t2 --\u003e t2e t2e --\u003e Concat level --\u003e Concat MaxHp --\u003e Concat Attack --\u003e Concat Defense --\u003e Concat Special --\u003e Concat Moves --\u003e MovesE MovesE --\u003e Concat Concat --\u003e Linear1 Linear1 --\u003e Linear2 Linear2 --\u003e Flatten1 Screen Network --- config: theme: mc look: handDrawn --- flowchart LR Concat2(\"Concat\") gamescreen(\"Game Screen\n72x80x1\ngrayscale\") visitedmask(\"Visited Mask\n72x80\") relu1(\"ReLU\") Conv1(\"2D CNN\n72x80x2\") relu2(\"ReLU\") Conv2(\"2D CNN\n72x80x32\") relu3(\"ReLU\") Conv3(\"2D CNN\n36x40x64\") relu4(\"ReLU\") Conv4(\"2D CNN\n18x20x64\") Flatten2(\"Flatten\") gamescreen --\u003e Concat2 visitedmask --\u003e Concat2 Concat2 --\u003e Conv1 Conv1 --\u003e relu1 relu1 --\u003e Conv2 Conv2 --\u003e relu2 relu2 --\u003e Conv3 Conv3 --\u003e relu3 relu3 --\u003e Conv4 Conv4 --\u003e relu4 relu4 --\u003e Flatten2 Missing Events --- config: theme: mc look: handDrawn --- flowchart TB Rival3(\"Rival 3 Defeated Boolean\") Lapras(\"Lapras Acquired Boolean\") SaffronGuard(\"Drink Given Saffron Guard Boolean\") GameCornerRocket(\"Game Corner Rocket\") Let’s summarize what the observation consists of:\nObservation Shape Data Type Screen 72x80x1 int Visited Mask 72x80x1 int Map ID 1x1 int Blackout Map ID 1x1 int Item IDs 20x1 int Item Quantities 20x1 int Agent Party 6x11 int Events Array 1x2560 boolean Direction 1x1 int Current Battle Condition 1x1 int Rival 3 defeated 1x1 boolean Lapras Acquired 1x1 boolean Saffron Guard 1x1 boolean Game Corner Rocket Defeated 1x1 boolean Number of Safari Steps Remaining 1x1 int In the policy, I occasionally normalize the value by some constant. In machine learning, it is generally adviseable represent these numbers as values between 0 and 1 for model stability.\nThe CNN # The screen obs and visited mask were concatenated together to make 2 visual “channels”. These channels were passed to a 2D Convolutional Neural Network (CNN). The kernel sizes of the CNN were designed with the GameBoy\u0026rsquo;s in mind.\nThe first layer of the CNN used an 8x8 kernel mapping to the size of a gameboy tile with a stride of size 2 so inter-tile dependencies could be detected. In successive layers, I decreased the kernel size so as not to capture too much information and similarly stride by 2 each layer so that the agent can hopefully get a good sense for edge detection.\nOne-Hot encoding # One hot encoding is a convenient technique to take a value representing a category and map it to a representation useful for the model. It\u0026rsquo;s useful when the number of categories is low.\nOf the observations, direction, battle state (in-battle, wild battle, trainer battle) are represented as one-hot encoded values.\nEmbeddings # The map ID and blackout map IDs are passed to the policy as integers, but in the policy, I transform them them with an embedding layer. Embedding layers are a convenient way to handle decreasing the number of dimensions of a categorical input. Instead of a one hot encoding the map id with # map id channels, I only require 5 floats to accurately represent the map id space. I chose 5 based on a recommendation from Google\u0026rsquo;s Machine Learning Crash Course. Google recommends using # of rows^.25 for the number of dimensions in the embedding layer.\nItems held in the agent\u0026rsquo;s bag are also identified by ID. The Item IDs are passed to their own embedding layer. I scale the item embeddings by the item\u0026rsquo;s quantities; a number between 0 and 1 where 0 maps to 0 and 1 maps the max number of the same item an agent can have.\nParty Network # All party data is concatenated together and passed through a small dense layer to create a “Pokémon” space.\nBinary Vectors # In RAM, events are a packed binary vector with each bit representing one in-game completed event. I unpack this vector and pass it on to the policy. The event vector in RAM does not include Lapras, Rival 3 and the Saffron Guard. I pass these in separately as they are \u0026ldquo;event\u0026rdquo;-like in my opinion.\nSafari Steps # The num of steps left in the Safari Zone is in the range [0, 502]. I normalize the steps observation to a value between 0 and 1 where 0 means no steps are left and 1 means you have the max number of steps remaining.\nFinal Model Layers # Once all features have been transformed, all non-batch dimension data is flattened, concatenated and passed through a final linear layer before heading to the LSTM. The LSTM is an off the shelf component and not worth discussing.\n"},{"id":12,"href":"/pokerl/docs/chapter-3/running/","title":"Running","section":"Chapter 3: Building and Running the System","content":" Actually Building the RL System # So I have a policy, observations, an environment and a reward. Now it\u0026rsquo;s time to discuss the meat of the system and how I wrote it. Along the way, I\u0026rsquo;ll provide relevant snippets of code and tricks I did to make my system run as fast as possible.\nBefore I begin though, I am obligated to give thanks to Joseph Suarez, the creator of PufferAI and PufferLib. Joseph graciously donated 4 machines to this effort. Each machine contained a NVIDIA 4090 GPU, Intel Core i9-14900K, 125 GB of RAM and 2TB of disk space. Before his contribution, I was paying $200/month using vast.ai for training with worse hardware.\nGood hardware although not necessary, massively sped up training.\nBut what was more important than good hardware? Good software! I want to discuss some of the challenges of the Pokémon environment and mitigations I had in place for those challenges. In the end, I 10x’d the steps per second (sps) of training to a peak of 10000 sps with a lot of extra engineering effort. Potentially more effort than I put into everything I mentioned previously.\nPokémon is CPU Bound # The Emulator # Before I started working on the RL side of this project, it was commonly voiced in the Pokémon RL Discord that PyBoy slow. PyBoy happens to provide a very convenient interface for RL and is relatively well performant for Python. However, using an emulator comes with its downsides. Namely, PyBoy is emulating every instruction from the ROM. That means no compiler optimizations, no link time optimizations etc. I could have attempted to rewrite Pokémon in a modern language, but that would have been a Herculean effort on its own. Instead, PyBoy got faster.\nA slow PyBoy was the case. But I’ve been in frequent communication with the creator of PyBoy for a while and we (mostly him) have put a lot of effort into improving PyBoy since late 2023. PyBoy has released a number of updates that have improved runtime speed and developer ergonomics dramatically including:\nBetter usage of compiler optimization flags. Better usage of Cython’s gil release options. JIT compiling the ROM. Data layout optimization. The addition of hooks. Most of these changes are in PyBoy’s core. Hooks are a special addition that let me inject Python code at specified instructions. Although the context switch back to Python slows down execution, the less frequent RAM reads hooks allow have led to an overall speedup in Pokémon.\nCollecting Screen Data # With the working emulator, did I need to collect data every frame? Short answer, no. In fact, it is suboptimal to do so. Pokémon only uses action information sparingly. This is well documented in Peter Whidden’s video. In order to use PyBoy effectively:\nI render the game headless. No UI means less CPU time spent rendering the game. For every action, I tick PyBoy 24 times and record the game screen on the last step. I don’t collect information from memory every step. def run_action_on_emulator(self, action): self.action_hist[action] += 1 # press button then release after some steps # TODO: Add video saving logic if not self.disable_ai_actions: self.pyboy.send_input(VALID_ACTIONS[action]) self.pyboy.send_input(VALID_RELEASE_ACTIONS[action], delay=8) self.pyboy.tick(self.action_freq - 1, render=False) # TODO: Split this function up. update_seen_coords should not be here! self.update_seen_coords() while self.read_m(\u0026#34;wJoyIgnore\u0026#34;): # DO NOT DELETE. Some animations require dialog navigation self.pyboy.button(\u0026#34;a\u0026#34;, 8) self.pyboy.tick(self.action_freq, render=False) # One last tick just in case self.pyboy.tick(1, render=True) If we look at the core functionality for running an action on the emulator, there is one small addition. Ticking while the agent does not have control. Waiting for control to return to the player removes certain levels of non-determinism. Most annoyingly, the extra animation when moving a boulder with strength.\nWhen to Care About the GPU # Generating data on the CPU has generally been the bottleneck, but training and inference do occur and during the training loop of my RL system, I am not running the emulator. I’m running the policy on the GPU with PyTorch. There are still gains to be had from speeding up the GPU side of training, even if it didn’t give me 2x returns.\nThere are many avenues for speeding up training:\nDecrease the model size. Decrease the data size. Improve the GPU utilization. Improve sample efficiency. There are some common techniques to improve the speedup. Some I contributed back to PufferLib (since early 2024 I have optimized pieces of PufferLib for a 2x speed up). I achived a 30% GPU speed improvement on inference by using torch.compile improved the GPU utilization with torch.compile. Torch compilation traces the execution graph of a PyTorch module and will create an optimized GPU execution graph.\nThe other major speed optimization in the training loop came from making sure I pinned memory for any tensors I used on GPU. Pinning prevents an extra memory copy when moving data from the CPU host to the GPU device.\nObservation Size Matters # After all that, the biggest bottleneck happened to be the data transfer of the observation. Not just from the CPU to GPU, but also from each agent back to the buffer holding all agent experiences for the current policy.\nRemember the model is 40MB. However, model input for a ≈20k row batch is ≈450MB.\nI looked into ways to decrease the memory size of the batch I sent to the GPU! Some techniques I used were really neat and I want to brag about them beyond choosing smaller data types, such as using a uint8 over a uint64 (1/8 smaller)\nCompressing Data # The GameBoy playable area is 144x160 pixels. PyBoy always returns 4 channels, RGBA. However, GameBoy games are grayscale so I only need 1 channel.\n\u0026ndash;\u0026gt; 1/4 reduction in screen data size or 92kB \u0026ndash;\u0026gt; ≈23kB\nAdditionally, as Peter Whidden showed, the agent doesn\u0026rsquo;t need the full screen. Half resolution or downscaling the image to 72x80 pixels is fine.\n\u0026ndash;\u0026gt; 1/4 reduction in screen data or 23kB \u0026ndash;\u0026gt; ≈6kB\nI can go one step further. GameBoy’s color system is 2 bit. Every pixel takes 2 bits representing one of 4 values. Even though PyBoy does not return the dense layout of the game screen, I could recompress the data such that every 8 bits, that is 1 byte, represents 4 pixels? Then, I could decompress the data on the GPU which should be fast since GPUs are designed for massively parallel operations.\n\u0026ndash;\u0026gt; Another 1/4 reduction in data size or 6kB \u0026ndash;\u0026gt; ≈2kB\nAnd there I had a potential near 64x speed up in host\u0026lt;-\u0026gt;device communication. Now why does this work?\nTo unpack on the GPU, I store two buffers, a bit mask and a number of shifts. I shift the bytes then apply the bit mask and voila hyper parallel decoding for both the visited mask and the game screen.\nclass Env(gym.Env): def collect_screen_data(self): # (144, 160, 3) game_pixels_render = np.expand_dims(self.screen.ndarray[:, :, 1], axis=-1) ... if self.reduce_res: game_pixels_render = game_pixels_render[::2, ::2, :] # game_pixels_render = skimage.measure.block_reduce(game_pixels_render, (2, 2, 1), np.min) game_pixels_render = ( ( np.digitize( game_pixels_render.reshape((-1, 4)), PIXEL_VALUES, right=True ).astype(np.uint8) \u0026lt;\u0026lt; np.array([6, 4, 2, 0], dtype=np.uint8) ) .sum(axis=1, dtype=np.uint8) .reshape((-1, game_pixels_render.shape[1] // 4, 1)) ) ... class Policy(nn.Module): def __init__(self): ... self.register_buffer( \u0026#34;screen_buckets\u0026#34;, torch.tensor(PIXEL_VALUES, dtype=torch.uint8), persistent=False ) self.register_buffer( \u0026#34;unpack_mask\u0026#34;, torch.tensor([0xC0, 0x30, 0x0C, 0x03], dtype=torch.uint8), persistent=False, ) self.register_buffer( \u0026#34;unpack_shift\u0026#34;, torch.tensor([6, 4, 2, 0], dtype=torch.uint8), persistent=False ) def unpack_screen_data(self, screen: torch.Tensor): screen = torch.index_select( self.screen_buckets, 0, ((screen.reshape((-1, 1)) \u0026amp; self.unpack_mask) \u0026gt;\u0026gt; self.unpack_shift).flatten().int(), ).reshape(restored_shape) visited_mask = torch.index_select( self.linear_buckets, 0, ((visited_mask.reshape((-1, 1)) \u0026amp; self.unpack_mask) \u0026gt;\u0026gt; self.unpack_shift) .flatten() .int(), ).reshape(restored_shape) This data technique also works for the events vector. Instead of representing every event flag as 1 byte, I sent the entire events vector to the GPU. Therefore 1 byte held 8 flags. On the GPU, and performed a similar shift and mask! Fast decoding of over 2000 event flags.\nA Good Vectorized Env is Worth its Weight in Gold # In between the emulator and the model, I had the code managing the vectorized environment. Originally, I used the library Stable Baselines 3. It’s a great library for prototyping with RL, but it handles env vectorization suboptimally. SB3 will wait for all environments to finish their current actions before sending the next batch.\nIn early 2024, I adopted PufferLib from PufferAI. PufferLib provides an asynchronous vectorized environment implementation. My PufferLib-based implementation collects data from environments until enough examples have been collected. Once enough examples have been collected, a few epochs of training occurs.\nGood Hyperparameters can mean faster training # The last dimension for a faster training experience isn’t making the overall iteration time faster, but improving sample efficiency. Once I had a game winning model, I spent over a month sweeping hyperparameter to find parameters that would produce a winning run in the fastest amount of time. Hyperparameters define any configurable part of the system. These are parameters that are not intended to be learned.\nMost hyperparameter sweep libraries will optimize for a single metric, such as reward. I wanted to optimize for reward with a time penalty. CARBS from Imbue is a cost-aware hyperparameter sweep tool. With proper hyperparameters, I reduced a single training run (with scripting) from 2 days to 7 hours. A nearly 7x reduction in training time.\n"},{"id":13,"href":"/pokerl/docs/chapter-3/reading-ram/","title":"Reading RAM","section":"Chapter 3: Building and Running the System","content":" Reading RAM # "},{"id":14,"href":"/pokerl/docs/chapter-3/metrics/","title":"Metrics and Visualization","section":"Chapter 3: Building and Running the System","content":" Metrics # "},{"id":15,"href":"/pokerl/docs/chapter-3/swarm/","title":"The Swarm","section":"Chapter 3: Building and Running the System","content":" The Swarm # I know I haven’t gotten to the final run yet. I have not mentioned an important modification I made to the normal PPO loop. Pokémon is a nearly open world game. Unfortunately, the experiential data can get “non-coherent” and the agents can become detached from each other.\nWithout sufficiently coherent data, the policy will not improve. This plagued me for months. Eventually, I took inspiration from the Go-Explore paper. I changed the way training is done. I made assurances that the data would remain coherent.\nHow? I began recording the save state every time an agent met a required game event or collected a required item. I experimented with how to use this data and the simplest variant of my experiments worked. Every time an agent completed a required objective, every agent would load the save state from the agent that completed the new objective.\nUnfortunately, that meant the agent may not generalize and learn how to actually beat the game. To account for that, I reset the swarm every time the game is won. As mentioned in the beginning, traditionally, RL runs for multiple episodes of independent agents. Now, I run multiple very long episodes of cooperative agents.\n"},{"id":16,"href":"/pokerl/docs/chapter-4/results/","title":"Results","section":"Chapter 4: Concluding Thoughts","content":" Results and Concluding Thoughts # So how did this agent perform? Well, I now have an agent that beats Pokémon with a few caveats. The code is open sourced for anyone to play with. Currently, I\u0026rsquo;m still struggling to get a stable enough run to prove that I can beat it with all my scripts disabled. I have tried runs with each script individually removed so I know it can be done, but there are a couple of bugs that sometimes occur that I need to iron out.\nWhat did I learn? # I learned that reward shaping is super important. I learned that my initial belief was right. JRPGs are special and I still believe they are a stepping stone towards AGI.\nThere’s a lot more to learn though. There are still scripts to remove. What if I could have the agent strength puzzles? I think at this point, more compromises will have to be made and the small model will be insufficient. I’m curious what is possible with LLMs. Recently, Claude showed interesting results playing Pokémon Red with an LLM. Although Claude isn\u0026rsquo;t using RL to play the game and the agent was most likely trained with Pokémon data (and therefore not a zero-shot approach), I’m curious when an LLM will replace my entire approach. I wish I could remove the swarming technique and many of the in-game knowledge rewards I provided, especially event rewards, but I don’t think I can without making compromises on training speed.\nMap ID rewards are still my least favorite addition overall, but without the ability for the agent to understand in-game text I don’t know how they can be removed.\nBut finally, exploration really is super powerful, but hard to tune. I think more time should be placed on how to explore as opposed to exploiting it.\n"},{"id":17,"href":"/pokerl/docs/chapter-4/future/","title":"Future","section":"Chapter 4: Concluding Thoughts","content":" The Future: Gen II # So what comes next? Well I’ll continue working on this project and at some point plan to make a video of my work and obtain a recording of at least one full playthrough.\nBut also, I\u0026rsquo;m thinking of trying the Gen II games, Gold/Silver/Crystal. Gen II has a decompilation from PRET and Gen II is easier in one important way; HMs can be used by pressing A in the field instead of extra menu interactions.\nHowever, Gen II is extremely difficult in other ways. There are more HMs and we can’t rely on gift Pokémon for all of them (namely Surf, Waterfall and Whirlpool).\nI’ve thought of a run that I think will beat the game. With the Totodile starter and a caught Red Gyarados, the agent would have enough Pokémon to satisfy all HMs and enough Pokémon to beat the game up to Red.\nTo Be Continued!\n"}]